---
title: 'Nonpar Bayes - HW1'
author: 'Rongzhao Yan'
date: '02/27/2022'
output: 
    bookdown::pdf_document2:
      latex_engine: xelatex
      toc: False
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = F, message = F)
options(scipen = 10)
```

# Overview

In this project, the dirichlet process univariate gaussian mixture model (DPGMM) is implemented,
to be specific, the model has the following prior structure:

$$Y_i | \theta_i \overset{\mathrm{i.i.d}}{\sim} N(\theta_i, \sigma^2)$$
$$\theta_i | G \overset{\mathrm{iid}}{\sim} G$$
$$G \sim DP(M, G_0), G_0 = N(m, B)$$
$$m \sim N(m_0, D)$$
$$B \sim Inv-Gamma(a, b)$$
$$M \sim Gamma(c, d).$$

As the prior structure suggested, there are 7 hyper parameters:

$$\{\sigma^2, m_0, D, a, b, c, d\}$$

need to be specified.
There are are 7 variables need to be sampled:

$$\{\mathbf{s}, \boldsymbol{\theta^{*}}, m, B, M, \eta\},$$

where $\mathbf{s}$ is the component membership for each datapoint,
$\boldsymbol{\theta^*}$ is the center for each component,
$\eta$ is an auxiliary variable.


The gibbs sampler uses fast mixing to integrate out $\theta^*_j$
when sampling $\boldsymbol{s}$ the membership for each data point.

`Python` is used for the algorithm implementation. Only `numpy` is used
as the framework of scientific computing.

The implementation is encapsulated into a python class: `DPGMM`.
To speed up the performance, `numba.jitclass` is used to
compile the class `DPGMM` into machine code. We note that
some `numpy` functions are not supported to compile into
machine code by `numba`, which are then reimplemented by hand.

The python code will attached in the appendix, and the corresponding source code
will also be attached as a seperate file.

```{r, eval = F}
library(tidyverse)
library(reticulate)
use_python("/opt/homebrew/Caskroom/miniforge/base/bin/python")
```


```{r}
library(tidyverse)
load("post.starting.RData")
y <- as.numeric(y)
hypers <- posterior_sample[[1]][seq(2000, 10000, by = 5), ]
comps <- posterior_sample[[2]][seq(2000, 10000, by = 5), ]
theta <- posterior_sample[[3]][seq(2000, 10000, by = 5), ]
```

```{r,, eval = F}
dp <- import("DP")
np <- import("numpy")
DPGMM <- dp$DPGMM
```

# Algorithm Analysis

## Starting Example

```{r, eval = F}
y <- np$array(c(rnorm(50, 5, 1), rnorm(50, -4, 1), rnorm(50, 13, 1)))
id <- factor(c(rep(0, 50), rep(1, 50), rep(2, 50)))
sigma2 <- 1.0
m0 <- 1.0
D <- 1.0
a <- 1.0
b <- 1.0
c <- 1.0
d <- 1.0
initial_components <- 4
iters <- 10000


dpgmm <- DPGMM(
    y, sigma2, m0, D,
    a, b, c, d, initial_components, iters
)

posterior_sample <- dpgmm$sample()
save(y, id, posterior_sample, file = "post.starting.RData")
```

We will dive deep into a synthetic data example and see how our model performs. 

The datapoints are simulated from three components: $N(5, 1), N(-4, 1), N(13, 1)$,
each component has 50 data points. Figure \@ref(fig:startingexampley)
shows the data points with the color their class membership.

The hyperparameter setting in this problem is:

$$\sigma^2 = 1, m_0 = 1, D = 1, a = 1, b = 1, c = 1, d = 1,$$
the MCMC will run 10000 iterations with burn-in period of 2000
and only picking every 5th iteration, resulting in total of 1600 posterior points.

Figure \@ref(fig:startingexampleConv) examines the convergence of MCMC.
The left two figure shows the ACF of the $\theta_1$ and
the population mean $m$. The right plot shows the 2d density plot of
$\theta_1 vs \theta_{150}$ (they are points comming from different component)
and that of $\theta_1 vs \theta_2$ (coming from the same component).

We can see that the autocorrelation for the sample is very low.
The density plot shows that the MCMC chain has iterated over the entire
sample space well, and $\theta_1 vs \theta_{150}$ are not that correlated,
and $\theta_1 vs \theta_2$ are highly linear correlated,
both of which are under expectation.

```{r startingexampley, fig.align = 'center', fig.height=3, fig.cap = 'The jittered plot of the synthetic data with color its corresponding membership', out.width='1\\linewidth'}
ggplot() +
    geom_jitter(aes(x = "y", y = y, color = id)) +
    theme(axis.title.x = element_blank()) +
    labs(x = "data points", color = "the actual membership")

```

```{r, include = FALSE}

library(ggfortify)

theta1 <- theta[, 1]
m <- hypers[, 1]
theta2 <- theta[, 2]
theta150 <- theta[, 150]

acf1 <- acf(theta1)
acfm <- acf(m)
```

```{r startingexampleConv, fig.align = 'center', fig.height=4, fig.cap = 'The ACF and 2d density plot of the MC chain', out.width='1\\linewidth'}


cowplot::plot_grid(
    autoplot(acf1) + labs(x = "theta1"),
    autoplot(acfm) + labs(x = "m"),
    ggplot() +
        stat_density_2d(aes(
            x = theta1, y = theta150,
            fill = ..level..
        ),
        geom = "polygon"
        ) +
        scale_fill_gradient2(
            low = "#FFFAFA", mid = "#BBFFFF", high = "#483D8B"
        ),
    ggplot() +
        stat_density_2d(aes(
            x = theta1, y = theta2,
            fill = ..level..
        ),
        geom = "polygon"
        ) +
        scale_fill_gradient2(
            low = "#FFFAFA", mid = "#BBFFFF", high = "#483D8B"
        ),
    ncol = 2
)
```
Figure \@ref(fig:startingexampleAvgtheta) shows the mean of the posterior sample of
$\theta_j$ for each data point. 
For the left plot, both x axis and y axis is the observed value,
resulting in a straight line. 
For the right plot, the x axis is the mean of the posterior sample 
of $\theta_j$ for each data point, the y axis is the observer value,
the color is the actual membership for both plot.

We see that the DPGMM sucessfully finds
the "center" of each datapoint in the sense that mean of $\theta_j$
for datapoints of the same true component is really close to each other.


```{r startingexampleAvgtheta, fig.align='center', fig.height=4,fig.cap='Left: observed value vs observed value, Right: mean of theta vs observed value', out.width='1\\linewidth'}

theta_mean <- apply(theta, 2, mean)

cowplot::plot_grid(
    ggplot() +
        geom_point(aes(x = y, y = y, color = id)) +
        labs(
            x = "the observed value",
            y = "the observed value"
        ),
    ggplot() +
        geom_point(aes(x = y, y = theta_mean, color = id)) +
        labs(
            x = "mean of theta for each data point",
            y = "the observed value"
        )
)
```

Next, we will dive deep into one round of iteration to see
how the model cluster the data in a specific round.

Figure \@ref(fig:startingexampleAround) shows that in the round 8000,
the sampled membership and $\theta_j$ of the model. The x axis is the index of each data point,
the y axis is the sampled $\theta_j$ for each data point, 
the color of each point corresponding
to the **sampled** membership, the shape of each point
corresponding to the **actual** membership.

We can see that the membership of data points that are actually of component 0 and component 2
(corresponding to shape $\circ$ and $\square$) are correctly sampled.

But the membership of data points that are actually of component 1
(corresponding to shape $\triangle$) are not perfectly sampled.
We see that there are two very small estimated components consisting of only very few datapoints,
whose center (i.e $\theta_j^*$) are really close to the center of (actual) component 1.

```{r startingexampleAround, fig.align = 'center', fig.cap = 'The sampled membership and actuall membership in round 8000', out.width='1\\linewidth'}

ggplot() +
    geom_point(aes(
        x = 1:length(y), y = theta[1601, ],
        shape = id, color = factor(comps[1601, ])
    ), size = 1) +
    labs(
        shape = "actual component id",
        color = "sampled component id",
        x = "index", y = "mean of theta"
    )
```

This example shows us that DP has the potential to generate some very small components.
Figure \@ref(fig:startingexampleNoComp) shows us the barplot of the number of components for each round
of the MCMC. We can see that the mode is 4, and most
of the time the sampled number of components will be
in $[3,6]$.

```{r startingexampleNoComp, fig.align = 'center', fig.height=3, fig.cap = 'The barplot of the number of sampled membership', out.width='1\\linewidth'}
num_of_clusters <- apply(
    comps, 1,
    function(x) length(unique(x))
)

ggplot() +
    geom_bar(aes(x = num_of_clusters)) +
    scale_x_continuous(breaks = seq(1, 10, by = 1)) +
    labs(x = "number of components", y = "count")
```


Next We will examine the averaged within clusters variance (WCV) for each $k$
among all iterations, in comparison with the true within cluster variance
(the square difference of $y$ and its true center).

$$WCV_k = |\{K^{(j)} = k\}|^{-1}\sum_{\{j:K^{(j)}=k\}}\sum_{i=1}^n (y_i - \theta_i^{(j)})^2,$$

where $K^{(j)}$ is the number of componentes in $jth$ iteration.


```{r, startingexampleWCV}

freq_k <- tibble(k = num_of_clusters) |>
    group_by(k) |>
    summarise(freq = n())

get_WCV <- function(k, num_of_clusters) {
    iteration_with_K_eq_k <- which(num_of_clusters == k)

    wcv_at_each_row <- apply(
        theta[iteration_with_K_eq_k, ], 1,
        function(x) sum((x - y)^2)
    )


    sum(wcv_at_each_row)
}

freq_k <- freq_k |>
    mutate(avg_wcv = sapply(
        k,
        function(x) {
            get_WCV(
                k = x,
                num_of_clusters = num_of_clusters
            )
        }
    ) / freq)

get_true_wcv <- function(y, true_center, true_membership) {
    wcv_at_each_component <- sapply(
        true_center,
        function(x) sum((y[true_membership == x] - x)^2)
    )

    sum(wcv_at_each_component)
}


freq_k <- freq_k |>
    mutate(
        true_wcv = get_true_wcv(
            y,
            c(5, -4, 13),
            c(rep(5, 50), rep(-4, 50), rep(13, 50))
        ),
        reduced_proportion = 1 - avg_wcv / true_wcv
    )
knitr::kable(freq_k,
    caption = "The average WCV for different number of clusters among all iterations, and the true WCV (using the true center for each data point), and the the proportion of averaged WCV for each k to the true WCV"
)
```

We see that when the posterior sample yields $k = 3$ the true number of components,
the avg-WCV is mostly closed to the true WCV. When $k$ is increasing,
the avg-WCV are much smaller than true WCV which is not abnormal as
when you increasing the number of clusters, the WCV must decreased.

We see our model has a very good approximation to the underlying truth
if the sampled number of clusters is equal to the tue number of clusters.

```{r}
Rcpp::sourceCpp("adj.cpp")

```

Finally we examine the averaged co-clustered
adjacency matrix heatmap among all iterations.



```{r seHeatmap, fig.align = 'center', fig.height=4, fig.cap = 'The co-clustered probabilites heatmap among all iterations', out.width='1\\linewidth'}

adj = get_sum_adj(comps) / nrow(comps) 
adj = adj |>
    expand_adj() |>
    as_tibble()

ggplot(adj) +
    geom_tile(aes(x = V1, y = V2, fill = V3)) +
    scale_fill_gradient2(
        low = "#FFFAFA", mid = "#BBFFFF", high = "#483D8B"
    ) +
    labs(fill = "prob") +
    theme_void()

```

## Some Other Scenerios

Above example shows that the model has good performance
when number of clusters is small and size of data is medium.

We next examine three different scenerios:

1. there exists one components with small size of data.
2. there are many components in medium size of data.
3. there are many components in large size of data.


In scenerio 1, we assume data coming from 4 centers: $0, -5, 10, 5$,
with the first three having number of observations 50 and the last one having
number of observations 10.

In scenerio 2, we assume data coming from 10 centers: [-25, 25] with equal distance.
The number of observations for each class will be 20. 

In scenerio 3, we assume data coming from 10 centers: [-25, 25] with equal distance.
The number of observations for each class will be 100.

All the other settings are remained the same as the previous section, including hyperparameters setting and MCMC setting.


```{r, eval = F}
centers_1 <- c(0, -5, 10, 5)
centers_2 <- seq(-25, 25, length = 10)
centers_3 <- centers_2

num_of_clusters_1 <- c(50, 50, 50, 10)
num_of_clusters_2 <- rep(20, 10)
num_of_clusters_3 <- rep(100, 10)

membership_1 <- purrr::map2(centers_1, num_of_clusters_1, ~ rep(.x, .y)) |>
    unlist()
membership_2 <- purrr::map2(centers_2, num_of_clusters_2, ~ rep(.x, .y)) |>
    unlist()
membership_3 <- purrr::map2(centers_3, num_of_clusters_3, ~ rep(.x, .y)) |>
    unlist()

y_1 <- np$array(rnorm(160, mean = membership_1))
y_2 <- np$array(rnorm(200, mean = membership_2))
y_3 <- np$array(rnorm(1000, mean = membership_3))


sigma2 <- 1.0
m0 <- 1.0
D <- 1.0
a <- 1.0
b <- 1.0
c <- 1.0
d <- 1.0
initial_components <- 4
iters <- 10000


dpgmm_1 <- DPGMM(
    y_1, sigma2, m0, D,
    a, b, c, d, initial_components, iters
)

posterior_sample_1 <- dpgmm_1$sample()

dpgmm_2 <- DPGMM(
    y_2, sigma2, m0, D,
    a, b, c, d, initial_components, iters
)

posterior_sample_2 <- dpgmm_2$sample()

dpgmm_3 <- DPGMM(
    y_3, sigma2, m0, D,
    a, b, c, d, initial_components, iters
)

posterior_sample_3 <- dpgmm_3$sample()
```

```{r, eval = F}
save(y_1, y_2, y_3,
    posterior_sample_1, posterior_sample_2, posterior_sample_3,
    file = "post.sample.3.RData"
)
```

```{r}
load("post.sample.3.RData")

centers_1 <- c(0, -5, 10, 5)
centers_2 <- seq(-25, 25, length = 10)
centers_3 <- centers_2

num_of_clusters_1 <- c(50, 50, 50, 10)
num_of_clusters_2 <- rep(20, 10)
num_of_clusters_3 <- rep(100, 10)

membership_1 <- purrr::map2(centers_1, num_of_clusters_1, ~ rep(.x, .y)) |>
    unlist()
membership_2 <- purrr::map2(centers_2, num_of_clusters_2, ~ rep(.x, .y)) |>
    unlist()
membership_3 <- purrr::map2(centers_3, num_of_clusters_3, ~ rep(.x, .y)) |>
    unlist()

y_1 <- as.numeric(y_1)
y_2 <- as.numeric(y_2)
y_3 <- as.numeric(y_3)

hypers_1 <- posterior_sample_1[[1]][seq(2000, 10000, by = 5), ]
comps_1 <- posterior_sample_1[[2]][seq(2000, 10000, by = 5), ]
theta_1 <- posterior_sample_1[[3]][seq(2000, 10000, by = 5), ]


hypers_2 <- posterior_sample_2[[1]][seq(2000, 10000, by = 5), ]
comps_2 <- posterior_sample_2[[2]][seq(2000, 10000, by = 5), ]
theta_2 <- posterior_sample_2[[3]][seq(2000, 10000, by = 5), ]

hypers_3 <- posterior_sample_3[[1]][seq(2000, 10000, by = 5), ]
comps_3 <- posterior_sample_3[[2]][seq(2000, 10000, by = 5), ]
theta_3 <- posterior_sample_3[[3]][seq(2000, 10000, by = 5), ]
```

```{r}

theta_mean_1 <- apply(theta_1, 2, mean)
theta_mean_2 <- apply(theta_2, 2, mean)
theta_mean_3 <- apply(theta_3, 2, mean)
```

```{r ex2Avgtheta, fig.align='center', fig.height=8,fig.cap='Left: observed value vs observed value, Right: mean of theta vs observed value', out.width='1\\linewidth'}


y_vs_y <- function(y, membership, scenerio) {
    ggplot() +
        geom_point(aes(
            x = y, y = y,
            color = factor(as.integer(membership))
        )) +
        labs(
            x = paste0("observed value in scenerio ", scenerio),
            y = paste0("observed value in scenerio ", scenerio),
            color = "true center"
        ) +
        theme(
            axis.title = element_text(size = 8),
            legend.text = element_text(size = 8),
            legend.title = element_text(size = 8)
        )
}

y_vs_theta <- function(y, theta_mean, membership, scenerio) {
    ggplot() +
        geom_point(aes(
            x = y, y = theta_mean,
            color = factor(as.integer(membership))
        )) +
        labs(
            x = paste0("mean of theta for each point in scenerio ", scenerio),
            y = paste0("observed value in scenerio ", scenerio),
            color = ""
        ) +
        theme(
            axis.title = element_text(size = 8),
            legend.text = element_text(size = 8),
            legend.title = element_text(size = 8)
        )
}

cowplot::plot_grid(

    y_vs_y(y_1, membership_1, 1),
    y_vs_theta(y_1, theta_mean_1, membership_1, 1),
    y_vs_y(y_2, membership_2, 2),
    y_vs_theta(y_2, theta_mean_2, membership_2, 2),
    y_vs_y(y_3, membership_3, 3),
    y_vs_theta(y_3, theta_mean_3, membership_3, 3),
    nrow = 3
)
```


Figure \@ref(fig:ex2Avgtheta) shows the same contents
of plot as the that with figure \@ref(fig:startingexampleAvgtheta).
We can see that each mean $\theta_j$
is largely seperated compared to the observed value.
But as the distance between each true center is relatively
small in this 3 scenerios compared to previous section (from 9 to 5),
we see that there are also some data points have their
mean of $\theta$ a bit far away from the true center.


We also note that in scenerio 1, the minor class ($\theta^*_j = 5$)
actually has relatively good clustering result, only one
data point is far away from its true center,
the mean $\theta$ of others is really close to each other.

Next we examine the co-clustered probabilities heatmap among
all iterations in each of the scenerio, see in figure \@ref(fig:ex2Heatmap).

We see that in the 3 scenerios the output is relatively good,
despite that there are some of the noises. 
Especially in scenerio 1, there are two points from two componentes
are being identified within the same components 
(and seperated from other points) significantly, we note that
one of which coming from the minor class.

In other two scenerios, we see the same situation for only very few points
in some components.

```{r}
get_freq_k <- function(theta) {
    k_in_each_row <- apply(theta, 1, function(x) length(unique(x)))
    tibble(k = k_in_each_row) |>
        group_by(k) |>
        count()
}

freq_k_1 <- get_freq_k(theta_1)
freq_k_2 <- get_freq_k(theta_2)
freq_k_3 <- get_freq_k(theta_3)
```


```{r}

avg_adj_1 = get_sum_adj(comps_1) / nrow(comps_1)
avg_adj_2 = get_sum_adj(comps_2) / nrow(comps_1)
avg_adj_3 = get_sum_adj(comps_3) / nrow(comps_1)

```

```{r ex2Heatmap, dev = "png", dpi = 300, fig.align = 'center', fig.height=6.5, fig.cap = 'The co-clustered probabilities heatmap among iterations in 3 scenerios', out.width='1\\linewidth'}

plot_adj = function(adj) {

    adj = adj |>
        expand_adj() |>
        as_tibble()

    ggplot(adj) +
        geom_tile(aes(x = V1, y = V2, fill = V3)) +
        scale_fill_gradient2(
            low = "#FFFAFA", mid = "#BBFFFF", high = "#483D8B"
        ) +
        labs(fill = "prob") +
        theme_void()

}    

cowplot::plot_grid(

    plot_adj(avg_adj_1),
    plot_adj(avg_adj_2),
    plot_adj(avg_adj_3),
    ncol = 1

)

```


